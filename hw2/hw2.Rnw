\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\author{Eric Mittman}
\title{Assignment 2}

\begin{document}
<<setup, echo=FALSE>>=
opts_chunk$set(echo=FALSE, out.width = ".8\\textwidth", fig.height = 3, fig.pos="ht")
@
\maketitle
\begin{enumerate}
\item[2.2.] 
\begin{enumerate}
  \item
  A container of sand containing buried object of interest is being poured out. When half of the sand has been poured, the pouring stops to allow search for the object. The ``time to failure" is the time at which the object is emitted from the container.
  
  \item
  <<1b>>=
  library(ggplot2)
  library(gridExtra)
  cdf1 <- data.frame(x = c(0,15,30),
                    xend = c(15,30,45),
                    y = c(0,.5,.5),
                    yend = c(.5,.5,1))
  pdf1 <- data.frame(x = c(0,15,30),
                    xend = c(15,30,45),
                    y = c(1/30,0,1/30),
                    yend = c(1/30,0,1/30))
  p1 <- ggplot(cdf1, aes(x=x,xend=xend,y=y,yend=yend)) + geom_segment() + xlab("minutes") + ggtitle("sand cdf")
  p2 <- ggplot(pdf1, aes(x=x,xend=xend,y=y,yend=yend)) + geom_segment() + xlab("minutes")  +ggtitle("sand pdf")
 grid.arrange(p1,p2,nrow=1)
  @
  \item Since half the sand has been poured, defining $t_{.5}$ to be the 15 minutes, the first time at which half of the sand has been poured is sensible because that is the probability that the object is in the poured sand at 15 minutes.
  
  An alternative definition would be that the $p^{th}$ quantile is the largest value of $t$ such that $F(t) \le p$. This would lead to $t_{.5} = 30$ minutes.
\end{enumerate}


\item[2.6.] 
\begin{enumerate}
  \item \[f(t) = 1/2, \quad t\in (0,2)\]
  \[h(t) = 1/(2-t), \quad t \in (0,2)\]
  \item 
  \[1-\exp\left[ -\int_{0}^{t}\frac{1}{2-x}dx\right] = 1-\exp\left[\int_{2}^{2-t}\frac{1}{u}du\right] = 1-\exp\left[ \log\left(\frac{2-t}{2}\right) \right] = t/2=F(t)\]
  \item
  <<1c>>=
 cdf2 <- data.frame(x=0,xend=2,y=0,yend=1)
 quant2 <- data.frame(x=.8,y=0,xend=.8,yend=.4)
 lines2 <- data.frame(x=c(.8,0),y=c(0,.4),xend=c(.8,.8),yend=c(.4,.4))
 p1 <- ggplot(cdf2, aes(x=x,xend=xend,y=y,yend=yend)) + geom_segment(data = cdf2) + ggtitle("cdf for 2.6") + geom_point(data = quant2) + geom_segment(data = lines2, linetype="dashed") + geom_text(data = quant2, label = "t[.4]", hjust = -.2, parse=TRUE)
 
 area2 <- data.frame(x=c(0,.8),xend = c(0,0), yend = c(0,0), y = c(0,0), ymin=c(0,0),ymax=c(1/2,1/2))
 pdf2 <- data.frame(x=0,xend=2,y=1/2,yend=1/2)
 p2 <- ggplot(pdf2, aes(x=x,xend=xend,y=y,yend=yend)) + geom_segment() + ggtitle("pdf for 2.6") + geom_ribbon(data=area2,aes(ymin=ymin, ymax= ymax), alpha=.5) + geom_point(data = quant2)+ geom_text(data = quant2, label = "t[.4]", hjust = -.2, parse=TRUE)
 grid.arrange(p1,p2,nrow=1)
 @
  \item 
  <<1d, fig.cap = "The hazard function goes to infinity at $x=2$ because as the cdf reaches 1 at 2. As the remaining time to fail decreases to zero, then the conditional rate of failure must increase without bound.">>=
 haz2 <- function(x) 1/(2-x)
 p3 <- ggplot(data.frame(x=c(0,2)), aes(x)) +
   stat_function(fun = haz2,n=1000) + ylim(c(0,50)) +
   ggtitle("hazard for 2.6")
 p3
 @
 \item
 \[F(t)=p \iff t=2p \implies t_{.4}=.8\]
 \item
 Since the pdf is constant over the support of $T$,
 \[Pr(.1<T\le.2) = P(.8<T\le.9) = 1/2 \cdot .1 = .05\]
 \item
 \[Pr(.1<T\le.2|T>.1) = \frac{.05}{1-F(.1)}=\frac{.05}{.95} = \frac{1}{19}\]
  \[Pr(.8<T\le.9|T>.8) = \frac{.05}{1-F(.8)}=\frac{.05}{.6} = \frac{1}{12}\]
  These are the same as what we would get by the approximation in (2.1):
  \[h(.1)\times.1 = 1/19,\quad h(.8)\times.1 = 1/12\]
 \item
 The approximation is exact in this case because the density is constant. If the pdf was decreasing on $(t,t+\triangle t)$, then (2.1) would be biased high. It would be biased low if the pdf was increasing on the interval.
\end{enumerate}

\item[2.8]
\begin{enumerate}
  \item
  \[f(t) = e^{-t},\quad h(t) = \frac{e^{-t}}{e^{-t}}=1\]
  \item
  <<3b, fig.height=4>>=
 hzd3 <- function(x) 1
 exp_shade <- function(x, high, low) ifelse(x<=high & x>=low, dexp(x), NA)
 q3 <- -log(.9)
 quant3 <- data.frame(x=q3, y=0)
 problines <- data.frame(x=c(0,0,.1,.2),xend=c(.1,.2,.1,.2),
                         y=c(pexp(.1),pexp(.2),0,0),
                         yend=c(pexp(.1),pexp(.2),pexp(.1),pexp(.2)))
 lines3 <- data.frame(x=c(q3,0),y=c(0,.1),xend=c(q3,q3),yend=c(.1,.1))
 area3 <- data.frame(x=c(0,q3))
 p <- ggplot(data.frame(x=c(0,3)),aes(x))
 p1 <- p + stat_function(fun = dexp) + ggtitle("pdf for 2.8") + geom_point(data=quant3, aes(x=x, y=y)) + 
   stat_function(fun = exp_shade, geom="area", alpha = .5, args=list(high=q3, low=0))+ 
   geom_text(data = quant3, aes(y=y), label = "t[.1]", vjust = .5, parse=TRUE) + 
   stat_function(fun = exp_shade, geom="area", alpha = .5, fill="red", args = list(high=.2,low=.1))
 
 p2 <- p + stat_function(fun = pexp) + ggtitle("cdf for 2.8") + geom_point(data=quant3, aes(x=x, y=y)) + geom_segment(data=lines3, aes(x, y, xend=xend, yend=yend), linetype = "dashed")+ geom_text(data = quant3, aes(y=y),label = "t[.1]", hjust = -.2, parse=TRUE) + geom_segment(data=problines, aes(x,y,xend=xend,yend=yend), color="red",linetype="dotted")
 p3 <- p + stat_function(fun = hzd3) + ggtitle("hazard for 2.8")
 grid.arrange(p1,p2, p3,nrow=2)
 @
  \item
  \[F(t_p) = p \iff 1-p = \exp(-t_p) \iff t_p = -log(1-p)\]
  So $t_{.1} = -log(.9) \approx .10536$.
  \item
  \[F(.2)-F(.1) = e^{-.1}-e^{-.2} \approx 0.08611 \]
  Using (2.1) we get
  \[h(.1)\times .1 \times (1-F(.1)) = .1 \times e^{-.1} = 0.09048,\]
  which is an overapproximation, as discussed in ex. 2.6.
\end{enumerate}

\item[2.18(d)]
From (2.6),
\[1-p_i = Pr(T\le t_{i-1} \lor T > t_i | T>t_{i-1}) = Pr(T>t_i|T>t_{i-1})\]
Since $t_0=0$ and the support of $T$ is $(0,\infty)$, $1 - p_1 = Pr(T>t_1)$. It follows from the definition of conditional probability that
\[\prod_{j=1}^{i}(1-p_j) = Pr(T>t_1,...,T>t_i) = Pr(T>t_i) = S(t_i)\]

\item[3.6ab]
\begin{enumerate}
  \item
  <<5a, results = 'asis'>>=
 n <- 30
 table5 <- data.frame(t = c(564, 1104, 1321, 1933, 1965, 2345, 2578, 3122, 4467, 5918, 6623, 7885, 7912, 8156, 12229),
                      d = c(0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0),
                      r = c(3, 0, 2, 3, 3, 0, 2, 3, 0, 2, 0, 0, 2, 3, 2),
                      n = c(30, 27, 26, 24, 21, 18, 17, 15, 12, 11, 9, 8, 7, 5, 2))
 table5$p <- ifelse(table5$d>0, table5$d/table5$n, NA)
 table5$pc <- 1-table5$p
 table5$S <- NA
 table5$S[table5$d>0] <- cumprod(table5$pc[table5$d>0])
 table5$F <- 1 - table5$S
 library(xtable)
 print(xtable(table5, caption = "Computation of non-parametric estimate of F(t) for problem 3.6."), include.rownames = FALSE)
 @
 See Table. 1.
  \item
<<5b>>=
 qplot(x=t, y=F, geom="point", data = subset(table5, d>0)) + ggtitle("Nonparametric estimate of F(t), ex.3.6") + xlab("hours") + ylim(c(0,1)) + xlim(c(0,8000))
 @
\end{enumerate}

\item[3.18]
In the heat exchanger data, the failure times are interval censored because the inspections happen infrequently, whereas in the shock absorber data the failure times are practically exact due to continuous inspection.
In the first case there is no information in the data regarding the distribution of failures between inspection; in the second there is.
\item[2.15]
\begin{enumerate}
  \item Selecting the battery at random is fair.
  \item Selecting the battery with the most running time is fair.
  \item Selecting the battery with the least running time is fair.
  \item Selecting the battery with the lowest measured capacity is unfair.
  
  The last choice is unfair, because this battery is likely to fail sooner than other batteries. This strategy would bias analysis toward larger failure times.
  
Among the fair choices, selecting the oldest battery would be reasonable if infant mortality is of primary interest. Selecting the newest battery would be better if wear out is a focus of the experiment. Selecting a battery at random would lead to a wider variety of censoring times and might be a good choice if both wear out and infant mortality are interesting.
\end{enumerate}

\item[3.1]
\begin{enumerate}
<<data>>=
 ball <- c(17.88, 28.92, 33, 41.52, 42.12, 45.6, 48.4, 51.84, 51.96, 54.12, 55.56, 67.8, 68.64, 68.64, 68.88, 84.12, 93.12, 98.64, 105.12, 105.84, 127.92, 128.04, 173.4)
 n <- length(ball)
 phat <- sum(ball<75)/n
 se <- sqrt(phat*(1-phat)/n)
 @
  \item
  \[\hat{F}(75) = 15/23 \approx 0.65217\]
  \item
  \[L = \left\{1 + \frac{(23-15+1)\times 1.960116}{15}\right\}^{-1}=0.45956\]
  \[U = \left\{1 + \frac{23-15}{(15+1)\times 2.183212}\right\}^{-1}=0.81366\]
  \item
  \[L = 15/23 - 1.644854 \times \sqrt(\frac{15/23 \times 8/23}{23}) = 0.48882\]
  \[U = 15/23 + 1.644854 \times \sqrt(\frac{15/23 \times 8/23}{23}) = 0.815565\]
  \item
  The intervals are similar, with the primary difference being that conservative estimate is a bit wider overall, reaching a bit further to the low side. There seems to be enough data in this instance to consider the normal approximation to be adequate. However, given that the conservative estimate is close, why not use it since it guarantees the nominal coverage?
\end{enumerate}
\end{enumerate}
\end{document}